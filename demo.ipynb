{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an RNN, from scratch!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote my own RNN from scratch based on [Ryan's nice slides](https://github.com/alan-turing-institute/transformers-reading-group/blob/main/sessions/03-seq2seq-part-i/seq2seq_part1_hut23_robots_in_disguise.pdf), and this notebook is the result. Now, I'll guide you to do the same (but using a lot of my boilerplate, so you can just do the fun stuff!)\n",
    "\n",
    "This notebook assumes familiarity with `numpy` and how to multiply arrays. The reason is that we're gonna use my favourite deep learing library [`jax`](https://github.com/google/jax), which uses the same syntax as `numpy`, but we just `import jax.numpy as jnp` instead! Everything else is more or less the same :)\n",
    "\n",
    "The basic layout is as follows: I'm gonna show you the formula, and write the function signature with some type annotations as hints. You're gonna fill in the blanks! But don't panic: most of these functions are one or two-liners that I literally just copied from the slides.\n",
    "\n",
    "In these functions, you'll see type annotations like this a lot:\n",
    "```python3\n",
    "x: Float[Array, \"dim1 dim2\"]\n",
    "```\n",
    "What am I meaning here? This just says that we have a variable `x`, which is an array of floating-point values (hence the `Float`). The string `\"dim1 dim2\"` is syntax for the shape of the array, which would have two dimensions `dim1` and `dim2`, i.e. a matrix with `dim1` rows and `dim2` columns. They don't represent anything concrete until we actually instantiate the value of `x`, but paying attention to these dimensions will help you make sure your matrix-vector multiplications will work (remember the rules of matrix multiplication: this matrix could only multiply an object on the right with leading dimension `dim2`). Oh, and if you're wondering where this syntax comes from -- it's the library [`jaxtyping`](https://github.com/google/jaxtyping)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import re\n",
    "from jaxtyping import Array, Float, Int\n",
    "from equinox import Module\n",
    "from typing import Generator\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "# This is a container for all the free parameters of an RNN!\n",
    "# You can see the shapes of each attribute from the type annotations.\n",
    "# We have a couple of sizes: hidden_state, embedding, vocab\n",
    "# -> these represent the size of the hidden_state weights,\n",
    "#    the embedding matrix, and the vocabulary respecively.\n",
    "# This parameters object will be passed to most functions below:\n",
    "# e.g. access the output weights by calling `params.output_weights` etc.\n",
    "class Parameters(Module):\n",
    "    embedding_weights: Float[Array, \"hidden_state embedding\"]\n",
    "    hidden_state_weights: Float[Array, \"hidden_state hidden_state\"]\n",
    "    output_weights: Float[Array, \"vocab hidden_state\"]\n",
    "    hidden_state_bias: Float[Array, \"hidden_state\"]\n",
    "    output_bias: Float[Array, \"vocab\"]\n",
    "    embedding_matrix: Float[Array, \"embedding vocab\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the hidden state\n",
    "\n",
    "We'll work our way inside-out, and start with the hidden state update, assuming the existence of some quantities we'll go on to define later. \n",
    "\n",
    "Recall that we looked at RNNs for *language modelling*, where every word is turned into a one-hot vector (or \"token\"). We then multiply these one-hot words by an *embedding matrix* $E$, which multiplies the words to reduce the dimension of that long one-hot vector (=size of the whole vocabulary) to some specified lower dimensional representation (normally ~100). This embedded word is then used to update the hidden state $h$ of the RNN.\n",
    "\n",
    "Assume some hidden state $h^{(t-1)}$, and we're trying to go to hidden state $h^{(t)}$ given our next embedded word, and the set of free parameters. Using the slides, fill in this function -- for $\\sigma$, you can use the activation function `jax.nn.tanh`. Here's the slide as a reminder:\n",
    "\n",
    "![](images/main_rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_hidden_state(\n",
    "    embedding: Float[Array, \"embedding\"],\n",
    "    hidden_state: Float[Array, \"hidden_state\"],\n",
    "    params: Parameters,\n",
    ") -> Float[Array, \"hidden_state\"]:\n",
    "    return jax.nn.tanh(\n",
    "        params.hidden_state_weights @ hidden_state\n",
    "        + params.embedding_weights @ embedding\n",
    "        + params.hidden_state_bias\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "We return to the topic of embeddings. Usually, people use [pre-trained embeddings](https://huggingface.co/blog/getting-started-with-embeddings) for tasks like text generation, but we're taking the ambitious route of learning the embedding matrix jointly with the weights. It's likely that the results of this notebook would be much better if we used embeddings that already encoded information about language and the relationship between words (though, the vocabulary would then need to be much larger than what we use later, which just finds all the unique words in a text document).\n",
    "\n",
    "We've already described how to embed a word -- just multiply by the embedding matrix. Implement this in the following function for a single word (and we'll use `jax.vmap` again to broadcast this over a sentence for later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings(\n",
    "    one_hot_word: Float[Array, \"vocab\"], params: Parameters\n",
    ") -> Float[Array, \"embedding\"]:\n",
    "    return params.embedding_matrix @ one_hot_word\n",
    "\n",
    "\n",
    "embeddings_map = jax.vmap(make_embeddings, in_axes=(0, None))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing an output\n",
    "\n",
    "Once we've updated the hidden state, we're able to produce an output from this timestep! Remember that this approach to language modelling means defining a probability distribution across our vocabulary -- this means that for each word, we assign a number that represents how likely that word is to appear next; our output $\\mathbf{\\hat{y}^{(t)}}$ is then a vector of the same length as the vocabulary with a set of numbers in the range 0-1 assigned to each word.\n",
    "\n",
    "Using this knowledge, and referring to the slide, implement the output computation using the current hidden state $h^{(t)}$ and the RNN parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(\n",
    "    hidden_state: Float[Array, \"hidden_state\"], params: Parameters\n",
    ") -> Float[Array, \"vocab\"]:\n",
    "    return jax.nn.softmax(params.output_weights @ hidden_state + params.output_bias)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Now all that's left to produce output from the RNN is to compose the functions from above!\n",
    "\n",
    "I've pseudocoded the function already using comments -- see if you can fill it in. Note that the input to the RNN here is a sentence (i.e. a sequence of one-hot-encoded words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(\n",
    "    data: Float[Array, \"sentence vocab\"], params: Parameters, hidden_size: int\n",
    ") -> Float[Array, \"sentence vocab\"]:\n",
    "    # apply embeddings_map to create a vector of embeddings\n",
    "    embeddings = embeddings_map(data, params)  # [\"sentence embedding\"]\n",
    "\n",
    "    # initialize the hidden state with zeros\n",
    "    hidden_state = jnp.zeros((hidden_size,))\n",
    "\n",
    "    # for each word in the vector of embeddings:\n",
    "    #    update the hidden state\n",
    "    #    compute the output word using that hidden state and store it\n",
    "    # return the set of outputs\n",
    "    outputs = []\n",
    "\n",
    "    for word in embeddings:\n",
    "        hidden_state = update_hidden_state(word, hidden_state, params)\n",
    "        outputs.append(output(hidden_state, params))\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching the network through the loss (teacher forcing)\n",
    "\n",
    "What's a good metric to see if the language modelling is working correctly? Well, if we're predicting the next word in the sentence, and we happen to have access to the whole sentence, we can just see what probability the model assigns to the correct next word. Since that's something we want to maximise, but neural networks are usually trained to minimise the objective, we can do the common trick of taking the negative log of that quantity to serve as our \"loss\" function -- the thing that we expect to be small when we're doing well. Using this and the slide below, implement the loss function for our RNN.\n",
    "\n",
    "Note that you only need to do this for a single output word -- we'll use the function `jax.vmap` to automatically vectorise the function to handle whole sentences!\n",
    "\n",
    "![](images/rnn_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(\n",
    "    output: Float[Array, \"vocab\"], next_one_hot_word: Float[Array, \"vocab\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    # index the softmax probs at the word of interest\n",
    "    return -jnp.log(output[jnp.argmax(next_one_hot_word)])\n",
    "\n",
    "\n",
    "sentence_loss = jax.vmap(loss, in_axes=(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(\n",
    "    data: Float[Array, \"sentence vocab\"],\n",
    "    next_words: Float[Array, \"sentence vocab\"],  # data shifted by 1 to the right\n",
    "    params: Parameters,\n",
    "    hidden_size: int,\n",
    ") -> Float[Array, \"\"]:\n",
    "    output = rnn(data, params, hidden_size)\n",
    "    return loss_map(output, next_words).mean(axis=0)\n",
    "\n",
    "\n",
    "loss_and_gradient = jax.value_and_grad(forward_pass, argnums=2)\n",
    "batched_grads = jax.jit(\n",
    "    jax.vmap(loss_and_gradient, in_axes=(0, 0, None, None)), static_argnums=(3,)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_sentence(\n",
    "    sentence: Int[Array, \"sentence\"], vocab_size: int\n",
    ") -> Int[Array, \"sentence vocab\"]:\n",
    "    return jnp.array([jnp.zeros((vocab_size,)).at[word].set(1) for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_words(\n",
    "    prompt: str,\n",
    "    vocab: list[str],\n",
    "    rnn_params: Parameters,\n",
    "    rnn_hidden_size: int,\n",
    "    num_predicted_tokens: int,\n",
    "    include_prompt=True,\n",
    ") -> str:\n",
    "    # Define a regular expression pattern to match all punctuation marks\n",
    "    punctuation_pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "    # Define a regular expression pattern to match words with apostrophes\n",
    "    apostrophe_pattern = r\"\\w+(?:\\'\\w+)?\"\n",
    "    # Define a regular expression pattern to match newlines\n",
    "    newline_pattern = r\"\\n\"\n",
    "\n",
    "    # Combine the three patterns to match all tokens\n",
    "    token_pattern = (\n",
    "        punctuation_pattern + \"|\" + apostrophe_pattern + \"|\" + newline_pattern\n",
    "    )\n",
    "\n",
    "    tokens = re.findall(token_pattern, prompt.lower())\n",
    "    one_hot_indicies = jnp.array([vocab.index(t) for t in tokens], dtype=jnp.int32)\n",
    "    sentence = one_hot_sentence(one_hot_indicies, len(vocab))\n",
    "    embeddings = embeddings_map(sentence, rnn_params)  # [\"sentence embedding\"]\n",
    "\n",
    "    hidden_state = jnp.zeros((rnn_hidden_size,))\n",
    "    outputs = [None] * num_predicted_tokens\n",
    "    for word in embeddings[:-1]:\n",
    "        hidden_state = update_hidden_state(word, hidden_state, rnn_params)\n",
    "    hidden_state = update_hidden_state(embeddings[-1], hidden_state, rnn_params)\n",
    "    outputs[0] = output(hidden_state, rnn_params)\n",
    "\n",
    "    for i in range(1, num_predicted_tokens):\n",
    "        embedded_pred = make_embeddings(outputs[i - 1], rnn_params)\n",
    "        hidden_state = update_hidden_state(embedded_pred, hidden_state, rnn_params)\n",
    "        outputs[i] = output(hidden_state, rnn_params)\n",
    "\n",
    "    res = jnp.array(outputs)\n",
    "    res_indicies = jnp.argmax(res, axis=1)\n",
    "    words = [vocab[i] for i in res_indicies]\n",
    "    out = \" \".join(words)\n",
    "    return prompt + \" | \" + out if include_prompt else out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 5.703, valid loss: 5.704\n",
      "train loss: 5.656, valid loss: 5.658\n",
      "train loss: 5.590, valid loss: 5.592\n",
      "train loss: 5.484, valid loss: 5.489\n",
      "train loss: 5.334, valid loss: 5.344\n",
      "train loss: 5.156, valid loss: 5.174\n",
      "train loss: 4.982, valid loss: 5.014\n",
      "train loss: 4.838, valid loss: 4.889\n",
      "train loss: 4.726, valid loss: 4.803\n",
      "train loss: 4.640, valid loss: 4.747\n",
      "train loss: 4.572, valid loss: 4.712\n",
      "train loss: 4.516, valid loss: 4.690\n",
      "train loss: 4.469, valid loss: 4.677\n",
      "train loss: 4.429, valid loss: 4.670\n",
      "train loss: 4.393, valid loss: 4.665\n",
      "train loss: 4.362, valid loss: 4.663\n",
      "train loss: 4.334, valid loss: 4.661\n",
      "train loss: 4.309, valid loss: 4.661\n",
      "train loss: 4.286, valid loss: 4.661\n",
      "train loss: 4.265, valid loss: 4.661\n",
      "train loss: 4.245, valid loss: 4.662\n",
      "train loss: 4.227, valid loss: 4.663\n",
      "train loss: 4.211, valid loss: 4.664\n",
      "train loss: 4.195, valid loss: 4.665\n",
      "train loss: 4.181, valid loss: 4.666\n",
      "train loss: 4.168, valid loss: 4.668\n",
      "train loss: 4.156, valid loss: 4.669\n",
      "train loss: 4.144, valid loss: 4.671\n",
      "train loss: 4.133, valid loss: 4.672\n",
      "train loss: 4.123, valid loss: 4.674\n",
      "train loss: 4.114, valid loss: 4.676\n",
      "train loss: 4.105, valid loss: 4.678\n",
      "train loss: 4.096, valid loss: 4.680\n",
      "train loss: 4.088, valid loss: 4.682\n",
      "train loss: 4.080, valid loss: 4.684\n",
      "train loss: 4.073, valid loss: 4.686\n",
      "train loss: 4.067, valid loss: 4.688\n",
      "train loss: 4.060, valid loss: 4.690\n",
      "train loss: 4.054, valid loss: 4.692\n",
      "train loss: 4.048, valid loss: 4.695\n",
      "train loss: 4.043, valid loss: 4.697\n",
      "train loss: 4.037, valid loss: 4.699\n",
      "train loss: 4.032, valid loss: 4.701\n",
      "train loss: 4.027, valid loss: 4.703\n",
      "train loss: 4.023, valid loss: 4.705\n",
      "train loss: 4.018, valid loss: 4.707\n",
      "train loss: 4.014, valid loss: 4.709\n",
      "train loss: 4.010, valid loss: 4.711\n",
      "train loss: 4.006, valid loss: 4.713\n",
      "train loss: 4.002, valid loss: 4.715\n",
      "train loss: 3.999, valid loss: 4.718\n",
      "train loss: 3.995, valid loss: 4.720\n",
      "train loss: 3.992, valid loss: 4.722\n",
      "train loss: 3.988, valid loss: 4.724\n",
      "train loss: 3.985, valid loss: 4.726\n",
      "train loss: 3.982, valid loss: 4.728\n",
      "train loss: 3.979, valid loss: 4.730\n",
      "train loss: 3.977, valid loss: 4.732\n",
      "train loss: 3.974, valid loss: 4.734\n",
      "train loss: 3.971, valid loss: 4.736\n",
      "train loss: 3.969, valid loss: 4.738\n",
      "train loss: 3.966, valid loss: 4.740\n",
      "train loss: 3.964, valid loss: 4.742\n",
      "train loss: 3.961, valid loss: 4.743\n",
      "train loss: 3.959, valid loss: 4.745\n",
      "train loss: 3.957, valid loss: 4.747\n",
      "train loss: 3.955, valid loss: 4.749\n",
      "train loss: 3.953, valid loss: 4.751\n",
      "train loss: 3.951, valid loss: 4.753\n",
      "train loss: 3.949, valid loss: 4.755\n",
      "train loss: 3.947, valid loss: 4.757\n",
      "train loss: 3.945, valid loss: 4.759\n",
      "train loss: 3.943, valid loss: 4.761\n",
      "train loss: 3.941, valid loss: 4.762\n",
      "train loss: 3.940, valid loss: 4.764\n",
      "train loss: 3.938, valid loss: 4.766\n",
      "train loss: 3.936, valid loss: 4.768\n",
      "train loss: 3.935, valid loss: 4.770\n",
      "train loss: 3.933, valid loss: 4.772\n",
      "train loss: 3.932, valid loss: 4.774\n",
      "train loss: 3.930, valid loss: 4.775\n",
      "train loss: 3.929, valid loss: 4.777\n",
      "train loss: 3.927, valid loss: 4.779\n",
      "train loss: 3.926, valid loss: 4.781\n",
      "train loss: 3.924, valid loss: 4.783\n",
      "train loss: 3.923, valid loss: 4.784\n",
      "train loss: 3.922, valid loss: 4.786\n",
      "train loss: 3.920, valid loss: 4.788\n",
      "train loss: 3.919, valid loss: 4.790\n",
      "train loss: 3.918, valid loss: 4.792\n",
      "train loss: 3.917, valid loss: 4.793\n",
      "train loss: 3.915, valid loss: 4.795\n",
      "train loss: 3.914, valid loss: 4.797\n",
      "train loss: 3.913, valid loss: 4.799\n",
      "train loss: 3.912, valid loss: 4.800\n",
      "train loss: 3.911, valid loss: 4.802\n",
      "train loss: 3.910, valid loss: 4.804\n",
      "train loss: 3.909, valid loss: 4.806\n",
      "train loss: 3.908, valid loss: 4.807\n",
      "train loss: 3.906, valid loss: 4.809\n",
      "best valid loss: 4.661\n",
      "Red fish  | \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "file_name = \"one-fish-two-fish.txt\"\n",
    "\n",
    "with open(file_name, \"r+\") as file:\n",
    "    all_text = file.read()\n",
    "    # all_text = all_text.replace('\\n', ' ').replace('  : ', '')\n",
    "\n",
    "# Define a regular expression pattern to match all punctuation marks\n",
    "punctuation_pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "# Define a regular expression pattern to match words with apostrophes\n",
    "apostrophe_pattern = r\"\\w+(?:\\'\\w+)?\"\n",
    "# Define a regular expression pattern to match newlines\n",
    "newline_pattern = r\"\\n\"\n",
    "\n",
    "# Combine the three patterns to match all tokens\n",
    "token_pattern = punctuation_pattern + \"|\" + apostrophe_pattern + \"|\" + newline_pattern\n",
    "\n",
    "\n",
    "# Split the text into tokens, including words with apostrophes as separate tokens\n",
    "all_words = re.findall(token_pattern, all_text.lower())\n",
    "vocab = list(set(all_words))\n",
    "\n",
    "sentence_length = 8  # even for now...\n",
    "\n",
    "vocab_one_hot_indicies = jnp.array([vocab.index(t) for t in all_words], dtype=jnp.int32)\n",
    "split_indicies = vocab_one_hot_indicies[\n",
    "    : (len(vocab) // sentence_length) * sentence_length\n",
    "].reshape(len(vocab) // sentence_length, sentence_length)\n",
    "# make last word random, shouldn't make too much of an impact (could be better handled with special char?)\n",
    "split_indicies_labels = jnp.concatenate(\n",
    "    (\n",
    "        vocab_one_hot_indicies[\n",
    "            1 : ((len(vocab) - 1) // sentence_length) * sentence_length\n",
    "        ],\n",
    "        jnp.array([0]),\n",
    "    )\n",
    ").reshape((len(vocab) - 1) // sentence_length, sentence_length)\n",
    "partition_index = 6 * int(len(split_indicies) / 7)\n",
    "train = split_indicies[:partition_index]\n",
    "train_labels = split_indicies_labels[:partition_index]\n",
    "valid = split_indicies[partition_index:]\n",
    "valid_labels = split_indicies_labels[partition_index:]\n",
    "\n",
    "batch_one_hot = jax.vmap(partial(one_hot_sentence, vocab_size=len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 400\n",
    "\n",
    "import numpy.random as npr\n",
    "\n",
    "\n",
    "def batches(training_data: Array, batch_size: int) -> Generator:\n",
    "    num_train = training_data.shape[0]\n",
    "    num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "    num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "    # batching mechanism, ripped from the JAX docs :)\n",
    "    def data_stream():\n",
    "        rng = npr.RandomState(0)\n",
    "        while True:\n",
    "            perm = rng.permutation(num_train)\n",
    "            for i in range(num_batches):\n",
    "                batch_idx = perm[i * batch_size : (i + 1) * batch_size]\n",
    "                yield train[batch_idx], train_labels[batch_idx]\n",
    "\n",
    "    return data_stream()\n",
    "\n",
    "\n",
    "batch = batches(train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 30\n",
    "h = 16\n",
    "v = len(vocab)\n",
    "o = v\n",
    "\n",
    "pars = Parameters(\n",
    "    embedding_weights=jax.random.truncated_normal(\n",
    "        lower=-0.1, upper=0.1, shape=[h, e], key=jax.random.PRNGKey(0)\n",
    "    ),\n",
    "    hidden_state_weights=jnp.identity(h),  # keep gradients from exploding\n",
    "    output_weights=jax.random.truncated_normal(\n",
    "        lower=-0.1, upper=0.1, shape=[o, h], key=jax.random.PRNGKey(0)\n",
    "    ),\n",
    "    hidden_state_bias=jnp.zeros((h,)),  # keep gradients from exploding\n",
    "    output_bias=jnp.zeros(\n",
    "        shape=[\n",
    "            o,\n",
    "        ]\n",
    "    ),\n",
    "    embedding_matrix=jax.random.truncated_normal(\n",
    "        lower=-0.1, upper=0.1, shape=[e, v], key=jax.random.PRNGKey(0)\n",
    "    ),\n",
    ")\n",
    "num_iter = 2000\n",
    "lr = 4e-2\n",
    "one_hot_valid, one_hot_valid_labels = batch_one_hot(valid), batch_one_hot(valid_labels)\n",
    "best_loss = 999\n",
    "best_pars = None\n",
    "\n",
    "\n",
    "def gradient_descent(param: jax.Array, grads: jax.Array) -> jax.Array:\n",
    "    return param - lr * grads.mean(axis=0)\n",
    "\n",
    "\n",
    "import optax\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip(1),\n",
    "    optax.adamw(learning_rate=lr),\n",
    ")\n",
    "opt_state = opt.init(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_iter):\n",
    "    sentences, sentence_labels = next(batch)\n",
    "    one_hot_sentences, one_hot_sentence_labels = batch_one_hot(\n",
    "        sentences\n",
    "    ), batch_one_hot(sentence_labels)\n",
    "    loss, grads = batched_grads(one_hot_sentences, one_hot_sentence_labels, pars, h)\n",
    "    valid_loss, _ = batched_grads(one_hot_valid, one_hot_valid_labels, pars, h)\n",
    "    loss, valid_loss = loss.mean(), valid_loss.mean()\n",
    "    pars = jax.tree_map(gradient_descent, pars, grads)\n",
    "    # avg_grads = jax.tree_map(lambda g: g.mean(axis=0), grads)\n",
    "    # updates, opt_state = opt.update(avg_grads, opt_state, params=pars)\n",
    "    # pars = optax.apply_updates(pars, updates)\n",
    "    if valid_loss < best_loss:\n",
    "        best_pars = deepcopy(pars)\n",
    "        best_loss = valid_loss\n",
    "    if i % 20 == 0:\n",
    "        print(f\"train loss: {loss.mean():.3f}\", end=\", \")\n",
    "        print(f\"valid loss: {valid_loss.mean():.3f}\")\n",
    "\n",
    "print(f\"best valid loss: {best_loss:.3f}\")\n",
    "print(predict_next_words(\"Red fish \", vocab, pars, h, 10, include_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello | \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(predict_next_words(\"Hello\", vocab, best_pars, h, 2, include_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
