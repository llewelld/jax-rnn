{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an RNN, from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import re\n",
    "from jaxtyping import Array, Float, Int\n",
    "from equinox import Module\n",
    "from typing import Generator\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class Parameters(Module):\n",
    "    embedding_weights: Float[Array, \"hidden_state embedding\"]\n",
    "    hidden_state_weights: Float[Array, \"hidden_state hidden_state\"]\n",
    "    output_weights: Float[Array, \"vocab hidden_state\"]\n",
    "    hidden_state_bias: Float[Array, \"hidden_state\"]\n",
    "    output_bias: Float[Array, \"vocab\"]\n",
    "    embedding_matrix: Float[Array, \"embedding vocab\"]\n",
    "\n",
    "\n",
    "def update_hidden_state(\n",
    "    embedding: Float[Array, \"embedding\"],\n",
    "    hidden_state: Float[Array, \"hidden_state\"],\n",
    "    params: Parameters,\n",
    ") -> Float[Array, \"hidden_state\"]:\n",
    "    return jax.nn.tanh(\n",
    "        params.hidden_state_weights @ hidden_state\n",
    "        + params.embedding_weights @ embedding\n",
    "        + params.hidden_state_bias\n",
    "    )\n",
    "\n",
    "\n",
    "def output(\n",
    "    hidden_state: Float[Array, \"hidden_state\"], params: Parameters\n",
    ") -> Float[Array, \"vocab\"]:\n",
    "    return jax.nn.softmax(params.output_weights @ hidden_state + params.output_bias)\n",
    "\n",
    "\n",
    "def loss(\n",
    "    output: Float[Array, \"vocab\"], next_one_hot_word: Float[Array, \"vocab\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    # index the softmax probs at the word of interest\n",
    "    return -jnp.log(output[jnp.argmax(next_one_hot_word)])\n",
    "\n",
    "\n",
    "loss_map = jax.vmap(loss, in_axes=(0, 0))\n",
    "\n",
    "\n",
    "def make_embeddings(\n",
    "    one_hot_word: Float[Array, \"vocab\"], params: Parameters\n",
    ") -> Float[Array, \"embedding\"]:\n",
    "    return params.embedding_matrix @ one_hot_word\n",
    "\n",
    "\n",
    "embeddings_map = jax.vmap(make_embeddings, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def rnn(\n",
    "    data: Float[Array, \"sentence vocab\"], params: Parameters, hidden_size: int\n",
    ") -> Float[Array, \"sentence vocab\"]:\n",
    "    embeddings = embeddings_map(data, params)  # [\"sentence embedding\"]\n",
    "\n",
    "    hidden_state = jnp.zeros((hidden_size,))\n",
    "    # outputs = []\n",
    "\n",
    "    # for word in embeddings:\n",
    "    #     hidden_state = update_hidden_state(word, hidden_state, params)\n",
    "    #     outputs.append(output(hidden_state, params))\n",
    "\n",
    "    def update_fn(carry, input):\n",
    "        hidden_state, params = carry\n",
    "        word = input\n",
    "        hidden_state = update_hidden_state(word, hidden_state, params)\n",
    "        out = output(hidden_state, params)\n",
    "        return (hidden_state, params), out\n",
    "\n",
    "    carry = (hidden_state, params)\n",
    "    _, outputs = jax.lax.scan(update_fn, carry, embeddings)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def forward_pass(\n",
    "    data: Float[Array, \"sentence vocab\"],\n",
    "    next_words: Float[Array, \"sentence vocab\"],  # data shifted by 1 to the right\n",
    "    params: Parameters,\n",
    "    hidden_size: int,\n",
    ") -> Float[Array, \"\"]:\n",
    "    output = rnn(data, params, hidden_size)\n",
    "    return loss_map(output, next_words).mean(axis=0)\n",
    "\n",
    "\n",
    "loss_and_gradient = jax.value_and_grad(forward_pass, argnums=2)\n",
    "batched_grads = jax.jit(\n",
    "    jax.vmap(loss_and_gradient, in_axes=(0, 0, None, None)), static_argnums=(3,)\n",
    ")\n",
    "\n",
    "\n",
    "def one_hot_sentence(\n",
    "    sentence: Int[Array, \"sentence\"], vocab_size: int\n",
    ") -> Int[Array, \"sentence vocab\"]:\n",
    "    return jnp.array([jnp.zeros((vocab_size,)).at[word].set(1) for word in sentence])\n",
    "\n",
    "\n",
    "def predict_next_words(\n",
    "    prompt: str,\n",
    "    vocab: list[str],\n",
    "    rnn_params: Parameters,\n",
    "    rnn_hidden_size: int,\n",
    "    num_predicted_tokens: int,\n",
    "    include_prompt=True,\n",
    ") -> str:\n",
    "    # Define a regular expression pattern to match all punctuation marks\n",
    "    punctuation_pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "    # Define a regular expression pattern to match words with apostrophes\n",
    "    apostrophe_pattern = r\"\\w+(?:\\'\\w+)?\"\n",
    "    # Define a regular expression pattern to match newlines\n",
    "    newline_pattern = r\"\\n\"\n",
    "\n",
    "    # Combine the three patterns to match all tokens\n",
    "    token_pattern = (\n",
    "        punctuation_pattern + \"|\" + apostrophe_pattern + \"|\" + newline_pattern\n",
    "    )\n",
    "\n",
    "    tokens = re.findall(token_pattern, prompt.lower())\n",
    "    one_hot_indicies = jnp.array([vocab.index(t) for t in tokens], dtype=jnp.int32)\n",
    "    sentence = one_hot_sentence(one_hot_indicies, len(vocab))\n",
    "    embeddings = embeddings_map(sentence, rnn_params)  # [\"sentence embedding\"]\n",
    "\n",
    "    hidden_state = jnp.zeros((rnn_hidden_size,))\n",
    "    outputs = [None] * num_predicted_tokens\n",
    "    for word in embeddings[:-1]:\n",
    "        hidden_state = update_hidden_state(word, hidden_state, rnn_params)\n",
    "    hidden_state = update_hidden_state(embeddings[-1], hidden_state, rnn_params)\n",
    "    outputs[0] = output(hidden_state, rnn_params)\n",
    "\n",
    "    for i in range(1, num_predicted_tokens):\n",
    "        embedded_pred = make_embeddings(outputs[i - 1], rnn_params)\n",
    "        hidden_state = update_hidden_state(embedded_pred, hidden_state, rnn_params)\n",
    "        outputs[i] = output(hidden_state, rnn_params)\n",
    "\n",
    "    res = jnp.array(outputs)\n",
    "    res_indicies = jnp.argmax(res, axis=1)\n",
    "    words = [vocab[i] for i in res_indicies]\n",
    "    out = \" \".join(words)\n",
    "    return prompt + \" | \" + out if include_prompt else out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 5.704, valid loss: 5.705\n",
      "train loss: 4.560, valid loss: 5.125\n",
      "train loss: 3.951, valid loss: 4.933\n",
      "train loss: 3.853, valid loss: 5.032\n",
      "train loss: 3.818, valid loss: 5.166\n",
      "train loss: 3.782, valid loss: 5.225\n",
      "train loss: 3.736, valid loss: 5.323\n",
      "train loss: 3.645, valid loss: 5.409\n",
      "train loss: 3.435, valid loss: 5.410\n",
      "train loss: 3.152, valid loss: 5.365\n",
      "train loss: 2.851, valid loss: 5.433\n",
      "train loss: 2.544, valid loss: 5.523\n",
      "train loss: 2.258, valid loss: 5.633\n",
      "train loss: 1.991, valid loss: 5.641\n",
      "train loss: 1.724, valid loss: 5.612\n",
      "train loss: 1.519, valid loss: 5.804\n",
      "train loss: 1.348, valid loss: 5.972\n",
      "train loss: 1.208, valid loss: 6.031\n",
      "train loss: 1.102, valid loss: 6.127\n",
      "train loss: 1.012, valid loss: 6.248\n",
      "train loss: 0.928, valid loss: 6.372\n",
      "train loss: 0.859, valid loss: 6.457\n",
      "train loss: 0.801, valid loss: 6.528\n",
      "train loss: 0.749, valid loss: 6.582\n",
      "train loss: 0.697, valid loss: 6.630\n",
      "train loss: 0.659, valid loss: 6.670\n",
      "train loss: 0.627, valid loss: 6.718\n",
      "train loss: 0.594, valid loss: 6.799\n",
      "train loss: 0.556, valid loss: 6.869\n",
      "train loss: 0.520, valid loss: 6.944\n",
      "train loss: 0.477, valid loss: 6.996\n",
      "train loss: 0.449, valid loss: 7.079\n",
      "train loss: 0.425, valid loss: 7.204\n",
      "train loss: 0.404, valid loss: 7.277\n",
      "train loss: 0.381, valid loss: 7.367\n",
      "train loss: 0.361, valid loss: 7.418\n",
      "train loss: 0.340, valid loss: 7.484\n",
      "train loss: 0.314, valid loss: 7.536\n",
      "train loss: 0.294, valid loss: 7.576\n",
      "train loss: 0.280, valid loss: 7.618\n",
      "train loss: 0.265, valid loss: 7.644\n",
      "train loss: 0.252, valid loss: 7.658\n",
      "train loss: 0.238, valid loss: 7.710\n",
      "train loss: 0.227, valid loss: 7.724\n",
      "train loss: 0.218, valid loss: 7.756\n",
      "train loss: 0.209, valid loss: 7.816\n",
      "train loss: 0.200, valid loss: 7.877\n",
      "train loss: 0.213, valid loss: 7.982\n",
      "train loss: 0.190, valid loss: 7.901\n",
      "train loss: 0.183, valid loss: 7.959\n",
      "train loss: 0.174, valid loss: 8.007\n",
      "train loss: 0.167, valid loss: 8.061\n",
      "train loss: 0.162, valid loss: 8.111\n",
      "train loss: 0.157, valid loss: 8.155\n",
      "train loss: 0.152, valid loss: 8.198\n",
      "train loss: 0.147, valid loss: 8.250\n",
      "train loss: 0.142, valid loss: 8.304\n",
      "train loss: 0.138, valid loss: 8.354\n",
      "train loss: 0.134, valid loss: 8.401\n",
      "train loss: 0.131, valid loss: 8.447\n",
      "train loss: 0.128, valid loss: 8.488\n",
      "train loss: 0.125, valid loss: 8.538\n",
      "train loss: 0.122, valid loss: 8.581\n",
      "train loss: 0.119, valid loss: 8.621\n",
      "train loss: 0.117, valid loss: 8.660\n",
      "train loss: 0.115, valid loss: 8.690\n",
      "train loss: 0.113, valid loss: 8.739\n",
      "train loss: 0.111, valid loss: 8.778\n",
      "train loss: 0.109, valid loss: 8.814\n",
      "train loss: 0.107, valid loss: 8.851\n",
      "train loss: 0.105, valid loss: 8.887\n",
      "train loss: 0.104, valid loss: 8.923\n",
      "train loss: 0.102, valid loss: 8.958\n",
      "train loss: 0.101, valid loss: 8.991\n",
      "train loss: 0.099, valid loss: 9.023\n",
      "train loss: 0.098, valid loss: 9.056\n",
      "train loss: 0.096, valid loss: 9.084\n",
      "train loss: 0.095, valid loss: 9.116\n",
      "train loss: 0.094, valid loss: 9.145\n",
      "train loss: 0.118, valid loss: 8.978\n",
      "train loss: 0.118, valid loss: 9.180\n",
      "train loss: 0.098, valid loss: 9.166\n",
      "train loss: 0.093, valid loss: 9.195\n",
      "train loss: 0.091, valid loss: 9.214\n",
      "train loss: 0.090, valid loss: 9.220\n",
      "train loss: 0.089, valid loss: 9.240\n",
      "train loss: 0.088, valid loss: 9.259\n",
      "train loss: 0.087, valid loss: 9.274\n",
      "train loss: 0.087, valid loss: 9.293\n",
      "train loss: 0.087, valid loss: 9.367\n",
      "train loss: 0.085, valid loss: 9.475\n",
      "train loss: 0.084, valid loss: 9.530\n",
      "train loss: 0.083, valid loss: 9.552\n",
      "train loss: 0.082, valid loss: 9.581\n",
      "train loss: 0.082, valid loss: 9.610\n",
      "train loss: 0.081, valid loss: 9.638\n",
      "train loss: 0.080, valid loss: 9.666\n",
      "train loss: 0.080, valid loss: 9.693\n",
      "train loss: 0.079, valid loss: 9.720\n",
      "train loss: 0.079, valid loss: 9.746\n",
      "best valid loss: 4.925\n",
      "Red fish  | , new fish . \n",
      " where do they have come\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "file_name = \"one-fish-two-fish.txt\"\n",
    "\n",
    "with open(file_name, \"r+\") as file:\n",
    "    all_text = file.read()\n",
    "    # all_text = all_text.replace('\\n', ' ').replace('  : ', '')\n",
    "\n",
    "# Define a regular expression pattern to match all punctuation marks\n",
    "punctuation_pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "# Define a regular expression pattern to match words with apostrophes\n",
    "apostrophe_pattern = r\"\\w+(?:\\'\\w+)?\"\n",
    "# Define a regular expression pattern to match newlines\n",
    "newline_pattern = r\"\\n\"\n",
    "\n",
    "# Combine the three patterns to match all tokens\n",
    "token_pattern = punctuation_pattern + \"|\" + apostrophe_pattern + \"|\" + newline_pattern\n",
    "\n",
    "\n",
    "# Split the text into tokens, including words with apostrophes as separate tokens\n",
    "all_words = re.findall(token_pattern, all_text.lower())\n",
    "vocab = list(set(all_words))\n",
    "\n",
    "sentence_length = 8  # even for now...\n",
    "\n",
    "vocab_one_hot_indicies = jnp.array([vocab.index(t) for t in all_words], dtype=jnp.int32)\n",
    "split_indicies = vocab_one_hot_indicies[\n",
    "    : (len(vocab) // sentence_length) * sentence_length\n",
    "].reshape(len(vocab) // sentence_length, sentence_length)\n",
    "# make last word random, shouldn't make too much of an impact (could be better handled with special char?)\n",
    "split_indicies_labels = jnp.concatenate(\n",
    "    (\n",
    "        vocab_one_hot_indicies[\n",
    "            1 : ((len(vocab) - 1) // sentence_length) * sentence_length\n",
    "        ],\n",
    "        jnp.array([0]),\n",
    "    )\n",
    ").reshape((len(vocab) - 1) // sentence_length, sentence_length)\n",
    "partition_index = 6 * int(len(split_indicies) / 7)\n",
    "train = split_indicies[:partition_index]\n",
    "train_labels = split_indicies_labels[:partition_index]\n",
    "valid = split_indicies[partition_index:]\n",
    "valid_labels = split_indicies_labels[partition_index:]\n",
    "\n",
    "\n",
    "batch_one_hot = jax.vmap(partial(one_hot_sentence, vocab_size=len(vocab)))\n",
    "batch_size = 400\n",
    "\n",
    "import numpy.random as npr\n",
    "\n",
    "\n",
    "def batches(training_data: Array, batch_size: int) -> Generator:\n",
    "    num_train = training_data.shape[0]\n",
    "    num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "    num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "    # batching mechanism, ripped from the JAX docs :)\n",
    "    def data_stream():\n",
    "        rng = npr.RandomState(0)\n",
    "        while True:\n",
    "            perm = rng.permutation(num_train)\n",
    "            for i in range(num_batches):\n",
    "                batch_idx = perm[i * batch_size : (i + 1) * batch_size]\n",
    "                yield train[batch_idx], train_labels[batch_idx]\n",
    "\n",
    "    return data_stream()\n",
    "\n",
    "\n",
    "batch = batches(train, batch_size)\n",
    "\n",
    "e = 30\n",
    "h = 16\n",
    "v = len(vocab)\n",
    "o = v\n",
    "\n",
    "pars = Parameters(\n",
    "    embedding_weights=jax.random.truncated_normal(\n",
    "        lower=-0.1, upper=0.1, shape=[h, e], key=jax.random.PRNGKey(0)\n",
    "    ),\n",
    "    hidden_state_weights=jnp.identity(h),  # keep gradients from exploding\n",
    "    output_weights=jax.random.truncated_normal(\n",
    "        lower=-0.1, upper=0.1, shape=[o, h], key=jax.random.PRNGKey(0)\n",
    "    ),\n",
    "    hidden_state_bias=jnp.zeros((h,)),  # keep gradients from exploding\n",
    "    output_bias=jnp.zeros(\n",
    "        shape=[\n",
    "            o,\n",
    "        ]\n",
    "    ),\n",
    "    embedding_matrix=jax.random.truncated_normal(\n",
    "        lower=-0.1, upper=0.1, shape=[e, v], key=jax.random.PRNGKey(0)\n",
    "    ),\n",
    ")\n",
    "num_iter = 2000\n",
    "lr = 4e-3\n",
    "one_hot_valid, one_hot_valid_labels = batch_one_hot(valid), batch_one_hot(valid_labels)\n",
    "best_loss = 999\n",
    "best_pars = None\n",
    "\n",
    "import optax\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip(1),\n",
    "    optax.adamw(learning_rate=lr),\n",
    ")\n",
    "opt_state = opt.init(pars)\n",
    "\n",
    "for i in range(num_iter):\n",
    "    sentences, sentence_labels = next(batch)\n",
    "    one_hot_sentences, one_hot_sentence_labels = batch_one_hot(\n",
    "        sentences\n",
    "    ), batch_one_hot(sentence_labels)\n",
    "    loss, grads = batched_grads(one_hot_sentences, one_hot_sentence_labels, pars, h)\n",
    "    valid_loss, _ = batched_grads(one_hot_valid, one_hot_valid_labels, pars, h)\n",
    "    loss, valid_loss = loss.mean(), valid_loss.mean()\n",
    "    # pars = jax.tree_map(lambda p, g: p-lr*g.mean(axis=0), pars, grads)\n",
    "    avg_grads = jax.tree_map(lambda g: g.mean(axis=0), grads)\n",
    "    updates, opt_state = opt.update(avg_grads, opt_state, params=pars)\n",
    "    pars = optax.apply_updates(pars, updates)\n",
    "    if valid_loss < best_loss:\n",
    "        best_pars = deepcopy(pars)\n",
    "        best_loss = valid_loss\n",
    "    if i % 20 == 0:\n",
    "        print(f\"train loss: {loss.mean():.3f}\", end=\", \")\n",
    "        print(f\"valid loss: {valid_loss.mean():.3f}\")\n",
    "\n",
    "print(f\"best valid loss: {best_loss:.3f}\")\n",
    "print(predict_next_words(\"Red fish \", vocab, pars, h, 10, include_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish | , two fish , red fish , red fish , red fish , red fish , red fish , red\n"
     ]
    }
   ],
   "source": [
    "print(predict_next_words(\"fish\", vocab, pars, h, 20, include_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
